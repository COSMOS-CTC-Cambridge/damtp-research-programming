Choosing an Algorithm
=====================

Difficult and Big Problems: Think Parallel
------------------------------------------

-   You wouldn't be on this course if you were dealing with easy, tiny problems
-   Parallelism is *nested*: you should try to exploit all levels

### First Level of Parallelism: SIMD

-   *SIMD* = Single Instruction Multiple Data
-   Large problems take too long without parallel processing [1]
    -   The Time of the Serial is over: no serial performance increase worth mentioning since 2005!
        -   In fact highest performing parallel processor's serial performance is only 40% of the 2005 level
    -   Parallel performance increase over serial performance **per core** (and per Hz) since 2005: **32x**!
    -   Even if you can wait at least 32 times longer, others won't and will scoop your result
    -   This performance provided by the SIMD capabilities of vector processing unit(s) (*VPU*)
        -   GPU and FPGA co-processors offer similar functionality

### Second level of parallelism: *multicore* and *manycore*

-   Gives another **10x** or so over 2005 performance
    -   Xeon Phi gives **30x**
-   SIMD and multicore together give 320$Ã—$/Hz (or 200x real) and Xeon Phi even 1000x!

### This performance does not come for free

-   Efficient use of SIMD needs careful programming
-   Efficient many-/multicore somewhat easier, but
-   Do not worry about it yet --- much can be gained from choosing a good library
-   But do plan for it from the beginning!

### Parallel processing should be part of program design

-   You should choose a parallelisable algorithm
    -   Example: mergesort is easy to parallelise, heapsort is very inefficient in parallel
-   It must be implemented such that it can exploit SIMD and many cores

OpenMP: a way to exploit the second level
-----------------------------------------

-   compiler-based approach to parallel programming (and some other things, too)
-   can also be used to offload to accelerators/GPUs
    -   unfortunately generally not in a portable way
-   needs a single, shared memory node to operate: use MPI to distribute across nodes, OpenMP to parallelise inside a node
-   but some nodes have non-uniform memory access, *NUMA*, which is tricky to use well
    -   using NUMA "wrong" will slow your code to a crawl
-   OpenMP provides some tools for exploiting SIMD as well, but generally should only be used as a last resort

### OpenMP and NUMA issues

-   OpenMP is shared memory model with implicit parallelism, so must be careful with what is shared and what is not
    -   false sharing (we'll return to this later)
    -   data corruption can occur
    -   code which is correct on one thread can suddenly be incorrect with OpenMP parallelism
    -   must use critical/atomic sections to guard shared written data
        -   effectively non-parallel region in an otherwise parallelised chunk
-   OpenMP can also deadlock
-   we have course material on this, can even run a course if needed
-   OpenMP is the oldest and really best suited for C/Fortran and simple parallelisation of loops
-   Depends on sometimes shaggy compiler-dependent runtime libraries
    -   Often find bugs in these, takes long to get them fixed
    -   Compilers support different parts of the standard: code portability is questionable

Modern ways to exploit the second level
---------------------------------------

### Threading Building Blocks (TBB)

-   Intel-sponsored Open Source project written in C++11
-   Parallelism comes from breaking problem into tasks which are executed in parallel
-   Much more capable in exploiting a node (whether NUMA or not) than OpenMP
    -   But a bit steeper learning curve
-   Has python bindings since version 4.4!
-   Has issues using threaded libraries
-   Can be used with accelerators

### dask and dask.distributed

-   Similar to TBB but entirely python
-   With `dask.distributed` you can go beyond a node, distributed (more on that in the next section)
-   Interactive in proper python style
-   Not much of a learning curve, but need to understand what a DAG is (but git is also a DAG, so you know this already!)
-   Very easy to set up and start using: can do single core on laptop etc
    -   Unfortunately not enough time to cover `dask` in this course
-   Can largely replace Map/Reduce and/or Apache Spark, but does not limit to those use cases and paradigms!
-   Has issues using threaded libraries
-   Can also be used with accelerators

### ipyparallel

-   A simpler, less powerful parallel processing tool
-   Still has a learning curve almost identical to dask
-   But we still use it on this course (shame on me!) for the *third* level of parallelism

### python has many other packages

-   threading (from standard library)
-   multiprocessing (also from standard library)
-   hard to decide which to learn

Think Distributed --- Parallel Taken to the Extreme
---------------------------------------------------

### The Third Level of Parallelism

-   Lash several, even millions, of machines (*nodes*) together using an *interconnect* to work together on the same problem
    -   top-1 (and I'm writing this just days before a new list will be publishes!) has 40960 nodes (machines) with 256+4 cores each (you do the maths)
    -   top-4 has 98304 nodes with 16+1 cores each
    -   trend is to increase cores/node and not so much \# nodes
-   Distribute problem data across nodes, each node working on its part of the problem
    -   Embarrassingly parallel: nodes work on different problems (possibly same equation with different parameters)
        -   MapReduce/Hadoop
        -   Apache Spark
-   The 32x and 1000x above can "now" become much bigger, even over 1 000 000x

Decrease time-to-solution: strong scaling
-----------------------------------------

-   assume an **ideal** world
-   now the inter-node communication becomes a bottle-neck: theoretical limit

![](images/domain_decomp_strong_scaling.png)

-   **conclusion**: even in an ideal world strong scaling eventually gives no benefit
    -   related to Amdahl's Law which we will meet soon
-   often load balancing becomes an issue well before the natural limit hits you

Increase problem size past single node: *weak scaling*
------------------------------------------------------

-   No more universal limit, in principle just add nodes to get bigger problem sizes
-   **algorithm** limits the scaling now
    -   Notoriously bad: Fast Fourier Transform
        -   Needs to transmit **volume**: in strong scaling surface was enough to kill scaling...
        -   Fast Multipole Method can often help here
        -   Fourier is nice on pen and paper, but not often a good choice on computer: consider *multigrid*
-   Load balancing is often problem here, too
    -   Many n-body codes suffer from this as even initially well balanced bodies tend to cluster and wreck the balance
    -   Task-based parallelism helps here, but often suffers from poor data locality: ways around that are complicated but exist (*QuickSched*)
-   Choice of a good algorithm again very, very important

Ensure scalability first
------------------------

-   As a rule of thumb, think what happens if your problem becomes 1000000 times larger
    -   Dealing with just 100 times larger is no easier
-   It's the hardest to deal with afterwards
-   Write smart, well designed code, optmimise on-core performance later
    -   But expect optimising will change the scaling characteristics (c.f. *Strong Scaling* --- the initial ratio will change and usually to the worse)
-   So, for now, we'll write distributed parallel code and worry about on-node and in-core performance later
-   Fortunately, a good choice of scientific library will deal with most distributed computing issues
    -   On this course, we will use *PETSc* but there are others, like
        -   SLEPc: everything you want from matrices, sparse or otherwise (depends on PETSc)
        -   ScaLAPACK: only ever use for matrices that cannot be stored in sparse format
        -   FEniCS: A Finite Element Method (FEM) solver library (lots of dependencies, including PETSc)
        -   ClawPACK/PyCLAW: A Finite Volume Method (FVM) solver library (you guessed it, depends on PETSc)
        -   Trilinos: the only real alternative to the PETSc family in very large scale frameworks, but very complex to install: contains 57 separate packages!
        -   FFTW: "the standard" FFT library, has shortcomings but (although you probably want to build your distributed version out of non-distributed 1D FFTs)

An aside: offload to an accelerator or GPU
------------------------------------------

-   Provides much higher performance/watt than normal CPU
    -   Knights Landing (and SW26010) have turned the tables
-   We do not have the time, but might do so in a later course!
-   Can be done explicitly, but almost never worth the effort: OpenMP, OpenCL and other methods provide at least a semblance of portability unlike explicit methods
-   Not so easy with python (but there's a solution)
-   GPU very popular in e.g. machine learning and neural networks

Exercises
---------

### Laplacian

-   write a function to compute the discrete Laplacian of some data
-   the function shall have three inputs:
    -   the data whose Laplacian is to be computed
    -   a variable to hold the resulting Laplacian
    -   a variable describing the discrete lattice
-   the function declaration shall be `def Laplacian(indata, result, d)`
-   remember the correctness tests
-   in jupyter/ipython you can use `%timeit Laplacian(X,Y,d)` to test your performance (but we'll deal with optimisation later)

### Simulated Annealing

-   write a function implementing the iterative step of the simulated annealing algorithm:
-   for those who do not know the algorithm, see <https://en.wikipedia.org/wiki/Simulated_annealing#Pseudocode>
-   the function shall have two inputs:
    -   the data to shuffle
    -   the current temperature
    -   objective function (this is called to compute the cost of the new configuration)
        -   the iterative step function will dictate the declaration of the objective function!
    -   auxiliary data (the objective function most likely needs some data more than just the data)
    -   if you want, you can optimise by passing also previous cost, so it does not need to be recalculated
-   the function declaration shall be `def SimAn(data, temperature, ObjFunc, aux_data, prev_temp=None)`
-   and the tests again, please
-   as practice you can also `add` and `commit` these to your local copy of the course repo, but recall that you will most likely `git reset --hard` at the beginning of the next lecture!

